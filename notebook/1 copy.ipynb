{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f36b2fe",
   "metadata": {},
   "source": [
    "#### Life cycle of Machine learning Project\n",
    "\n",
    "- Understanding the Problem Statement\n",
    "- Data Collection\n",
    "- Data Checks to perform\n",
    "- Exploratory data analysis\n",
    "- Data Pre-Processing\n",
    "- Model Training\n",
    "- Choose best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88462604",
   "metadata": {},
   "source": [
    "### 1) Problem statement\n",
    "- This project understands how the student's performance (test scores) is affected by other variables such as Gender, Ethnicity, Parental level of education, Lunch and Test preparation course.\n",
    "\n",
    "\n",
    "### 2) Data Collection\n",
    "- Dataset Source - https://www.kaggle.com/datasets/spscientist/students-performance-in-exams?datasetId=74977\n",
    "- The data consists of 8 column and 1000 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8b838",
   "metadata": {},
   "source": [
    "### 2.1 Import Data and Required Packages\n",
    "####  Importing Pandas, Numpy, Matplotlib, Seaborn and Warings Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e25ccdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1. IMPORTS AND SETUP\n",
    "# =====================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import calendar\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, MultiLabelBinarizer, StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56235f14",
   "metadata": {},
   "source": [
    "### 3. Data Checks to perform\n",
    "\n",
    "- Check Missing values\n",
    "- Check Duplicates\n",
    "- Check data type\n",
    "- Check the number of unique values of each column\n",
    "- Check statistics of data set\n",
    "- Check various categories present in the different categorical column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c70d5",
   "metadata": {},
   "source": [
    "### 4. Exploring Data ( Visualization )\n",
    "#### 4.1 Visualize average score distribution to make some conclusion. \n",
    "- Histogram\n",
    "- Kernel Distribution Function (KDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025593c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abcef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 2. LOAD DATA\n",
    "# =====================================================\n",
    "df = pd.read_csv('data/dataset.csv')\n",
    "df_copy = df.copy()\n",
    "\n",
    "# =====================================================\n",
    "# 3. CLEANING & PREPROCESSING\n",
    "# =====================================================\n",
    "# Clean strings (removes whitespace, leading/trailing underscores)\n",
    "for col in df_copy.columns:\n",
    "    if df_copy[col].dtype == 'object':\n",
    "        df_copy[col] = df_copy[col].str.strip().str.strip('_')\n",
    "\n",
    "# Specific column fixes\n",
    "df_copy['Customer_ID'] = df_copy['Customer_ID'].str.lstrip('CUS_0x')\n",
    "df_copy['Payment_Behaviour'] = df_copy['Payment_Behaviour'].replace('!@9#%8', np.nan)\n",
    "df_copy['Occupation'] = df_copy['Occupation'].replace('', np.nan).replace('___', np.nan)\n",
    "df_copy['Credit_Mix'] = df_copy['Credit_Mix'].replace('', np.nan)\n",
    "df_copy['Payment_of_Min_Amount'].replace('NM', 'No', inplace=True)\n",
    "\n",
    "# Month name to number\n",
    "month_to_num = {m:i for i,m in enumerate(calendar.month_name) if m}\n",
    "df_copy['Month'] = df_copy['Month'].map(month_to_num)\n",
    "\n",
    "# Drop irrelevant privacy columns\n",
    "df_copy.drop(columns=[c for c in [\"ID\",\"SSN\",\"Name\"] if c in df_copy.columns], inplace=True)\n",
    "\n",
    "# =====================================================\n",
    "# 4. TYPE CONVERSION & RANGE VALIDATION\n",
    "# =====================================================\n",
    "float_cols = ['Annual_Income','Interest_Rate', 'Monthly_Inhand_Salary', 'Changed_Credit_Limit',\n",
    "              'Outstanding_Debt','Credit_Utilization_Ratio','Total_EMI_per_month',\n",
    "              'Amount_invested_monthly','Monthly_Balance']\n",
    "\n",
    "int_cols = ['Age','Num_Bank_Accounts', 'Num_Credit_Card', 'Num_of_Loan',\n",
    "            'Delay_from_due_date','Num_of_Delayed_Payment','Num_Credit_Inquiries']\n",
    "\n",
    "for col in float_cols:\n",
    "    df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
    "\n",
    "for col in int_cols:\n",
    "    df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
    "\n",
    "# Drop out-of-range values \n",
    "for col in ['Age','Num_Bank_Accounts','Num_Credit_Card','Num_of_Loan',\n",
    "            'Num_of_Delayed_Payment','Num_Credit_Inquiries','Interest_Rate']:\n",
    "    df_copy.loc[(df_copy[col]<0)|(df_copy[col]>100), col] = np.nan\n",
    "df_copy.loc[(df_copy['Num_Bank_Accounts']>60)|(df_copy['Num_Bank_Accounts']<0), 'Num_Bank_Accounts'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "476ea768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Scientist', 'Teacher', 'Engineer', 'Entrepreneur', 'Developer',\n",
       "       'Lawyer', 'Media_Manager', 'Doctor', 'Journalist', 'Manager',\n",
       "       'Accountant', 'Musician', 'Mechanic', 'Writer', 'Architect'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy['Occupation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9981571e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "18697fc7-3649-42e6-a479-3fc5010be4f4",
       "rows": [
        [
         "Customer_ID",
         "12500"
        ],
        [
         "Occupation",
         "15"
        ],
        [
         "Type_of_Loan",
         "6260"
        ],
        [
         "Credit_Mix",
         "3"
        ],
        [
         "Credit_History_Age",
         "404"
        ],
        [
         "Payment_of_Min_Amount",
         "2"
        ],
        [
         "Payment_Behaviour",
         "6"
        ],
        [
         "Credit_Score",
         "3"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "Customer_ID              12500\n",
       "Occupation                  15\n",
       "Type_of_Loan              6260\n",
       "Credit_Mix                   3\n",
       "Credit_History_Age         404\n",
       "Payment_of_Min_Amount        2\n",
       "Payment_Behaviour            6\n",
       "Credit_Score                 3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1819e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 5. IMPUTATION (GROUPWISE)\n",
    "# =====================================================\n",
    "def impute_by_customer_median(df, columns):\n",
    "    group_medians = df.groupby('Customer_ID')[columns].median()\n",
    "    for col in columns:\n",
    "        df[col] = df[col].fillna(df['Customer_ID'].map(group_medians[col]))\n",
    "    return df\n",
    "\n",
    "def impute_by_customer_mode(df, columns):\n",
    "    for col in columns:\n",
    "        modes = df.groupby('Customer_ID')[col].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "        df[col] = df[col].fillna(df['Customer_ID'].map(modes))\n",
    "    return df\n",
    "\n",
    "df_copy = impute_by_customer_median(df_copy, float_cols + int_cols)\n",
    "cat_cols = df_copy.select_dtypes(include='object').columns.tolist()\n",
    "df_copy = impute_by_customer_mode(df_copy, [c for c in cat_cols if c!='Credit_History_Age'])\n",
    "\n",
    "# Impute global mode for 'Type_of_Loan'\n",
    "if 'Type_of_Loan' in df_copy.columns:\n",
    "    mode_value = df_copy['Type_of_Loan'].mode()[0]\n",
    "    df_copy['Type_of_Loan'] = df_copy['Type_of_Loan'].fillna(mode_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c395a43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "c47998a0-4e0c-4040-8c1e-6f0ae0753992",
       "rows": [
        [
         "Customer_ID",
         "0"
        ],
        [
         "Month",
         "0"
        ],
        [
         "Age",
         "0"
        ],
        [
         "Occupation",
         "0"
        ],
        [
         "Annual_Income",
         "0"
        ],
        [
         "Monthly_Inhand_Salary",
         "0"
        ],
        [
         "Num_Bank_Accounts",
         "0"
        ],
        [
         "Num_Credit_Card",
         "0"
        ],
        [
         "Interest_Rate",
         "0"
        ],
        [
         "Num_of_Loan",
         "0"
        ],
        [
         "Type_of_Loan",
         "0"
        ],
        [
         "Delay_from_due_date",
         "0"
        ],
        [
         "Num_of_Delayed_Payment",
         "0"
        ],
        [
         "Changed_Credit_Limit",
         "0"
        ],
        [
         "Num_Credit_Inquiries",
         "0"
        ],
        [
         "Credit_Mix",
         "0"
        ],
        [
         "Outstanding_Debt",
         "0"
        ],
        [
         "Credit_Utilization_Ratio",
         "0"
        ],
        [
         "Credit_History_Age",
         "9030"
        ],
        [
         "Payment_of_Min_Amount",
         "0"
        ],
        [
         "Total_EMI_per_month",
         "0"
        ],
        [
         "Amount_invested_monthly",
         "0"
        ],
        [
         "Payment_Behaviour",
         "0"
        ],
        [
         "Monthly_Balance",
         "1696"
        ],
        [
         "Credit_Score",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 25
       }
      },
      "text/plain": [
       "Customer_ID                    0\n",
       "Month                          0\n",
       "Age                            0\n",
       "Occupation                     0\n",
       "Annual_Income                  0\n",
       "Monthly_Inhand_Salary          0\n",
       "Num_Bank_Accounts              0\n",
       "Num_Credit_Card                0\n",
       "Interest_Rate                  0\n",
       "Num_of_Loan                    0\n",
       "Type_of_Loan                   0\n",
       "Delay_from_due_date            0\n",
       "Num_of_Delayed_Payment         0\n",
       "Changed_Credit_Limit           0\n",
       "Num_Credit_Inquiries           0\n",
       "Credit_Mix                     0\n",
       "Outstanding_Debt               0\n",
       "Credit_Utilization_Ratio       0\n",
       "Credit_History_Age          9030\n",
       "Payment_of_Min_Amount          0\n",
       "Total_EMI_per_month            0\n",
       "Amount_invested_monthly        0\n",
       "Payment_Behaviour              0\n",
       "Monthly_Balance             1696\n",
       "Credit_Score                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e4b5d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 6. CONVERT CREDIT HISTORY AGE TO MONTHS\n",
    "# =====================================================\n",
    "def convert_age_to_months(age_str):\n",
    "    if pd.isna(age_str) or age_str in ['NA', '_', '']:\n",
    "        return np.nan\n",
    "    match = re.match(r\"(\\d+)\\s*Years\\s*and\\s*(\\d+)\\s*Months\", str(age_str))\n",
    "    if match:\n",
    "        years = int(match.group(1))\n",
    "        months = int(match.group(2))\n",
    "        return years*12 + months\n",
    "    return np.nan\n",
    "\n",
    "if 'Credit_History_Age' in df_copy.columns:\n",
    "    df_copy['Credit_History_Age_Months'] = df_copy['Credit_History_Age'].apply(convert_age_to_months)\n",
    "    df_copy['Credit_History_Age_Months'] = df_copy['Credit_History_Age_Months'].fillna(df_copy['Credit_History_Age_Months'].median())\n",
    "    df_copy.drop(columns=['Credit_History_Age'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "67fd5c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature list: ['Month', 'Age', 'Occupation', 'Annual_Income', 'Monthly_Inhand_Salary', 'Interest_Rate', 'Payment_of_Min_Amount', 'Payment_Behaviour', 'Credit_Score', 'Auto Loan', 'Credit-Builder Loan', 'Debt Consolidation Loan', 'Home Equity Loan', 'Mortgage Loan', 'Not Specified', 'Payday Loan', 'Personal Loan', 'Student Loan', 'DTI', 'EMI_to_Income', 'Invest_to_Income', 'Balance_to_Income', 'Avg_Delay_if_Delayed', 'Has_Delays', 'High_Utilization', 'Total_Financial_Products', 'Inquiries_per_Year', 'Limit_Decrease_Flag', 'Large_Limit_Change', 'Num_Loan_Types']\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 7. ENCODING & FEATURE ENGINEERING\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "# MultiLabelBinarizer for loan types\n",
    "def split_loan_types(val):\n",
    "    if pd.isna(val): return []\n",
    "    return [loan.strip() for loan in str(val).replace(\" and \", \", \").split(\",\") if loan.strip()]\n",
    "if 'Type_of_Loan' in df_copy.columns:\n",
    "    df_copy[\"Type_of_Loan_List\"] = df_copy[\"Type_of_Loan\"].apply(split_loan_types)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    loan_dummies = pd.DataFrame(mlb.fit_transform(df_copy['Type_of_Loan_List']),\n",
    "                                columns=mlb.classes_,index=df_copy.index)\n",
    "    df_copy = pd.concat([df_copy, loan_dummies], axis=1)\n",
    "df_copy.drop(columns=['Type_of_Loan','Type_of_Loan_List'], inplace=True)\n",
    "\n",
    "\n",
    "# Feature engineering: ratios and flags\n",
    "df_copy['DTI'] = df_copy['Total_EMI_per_month'] / df_copy['Monthly_Inhand_Salary']\n",
    "df_copy['EMI_to_Income'] = df_copy['Outstanding_Debt'] / df_copy['Annual_Income']\n",
    "df_copy['Invest_to_Income'] = df_copy['Amount_invested_monthly'] / df_copy['Monthly_Inhand_Salary']\n",
    "df_copy['Balance_to_Income'] = df_copy['Monthly_Balance'] / df_copy['Monthly_Inhand_Salary']\n",
    "df_copy['Avg_Delay_if_Delayed'] = df_copy['Delay_from_due_date'] / df_copy['Num_of_Delayed_Payment'].replace(0, 1)\n",
    "df_copy['Has_Delays'] = (df_copy['Num_of_Delayed_Payment'] > 0).astype(int)\n",
    "df_copy['High_Utilization'] = (df_copy['Credit_Utilization_Ratio'] > 0.7).astype(int)\n",
    "df_copy['Total_Financial_Products'] = df_copy['Num_Bank_Accounts'] + df_copy['Num_Credit_Card'] + df_copy['Num_of_Loan']\n",
    "df_copy['Inquiries_per_Year'] = df_copy['Num_Credit_Inquiries'] / (df_copy['Credit_History_Age_Months']/12).replace(0, 1)\n",
    "df_copy['Limit_Decrease_Flag'] = (df_copy['Changed_Credit_Limit'] < 0).astype(int)\n",
    "df_copy['Large_Limit_Change'] = (df_copy['Changed_Credit_Limit'].abs() > 20).astype(int)\n",
    "loan_cols = [col for col in df_copy.columns if col.endswith(\"Loan\")]\n",
    "df_copy['Num_Loan_Types'] = df_copy[loan_cols].sum(axis=1)\n",
    "\n",
    "# Drop columns that have been fully replaced with engineered features\n",
    "drop_cols = ['Total_EMI_per_month','Outstanding_Debt','Amount_invested_monthly','Monthly_Balance',\n",
    "             'Delay_from_due_date','Num_of_Delayed_Payment','Credit_Utilization_Ratio','Num_Bank_Accounts',\n",
    "             'Num_Credit_Card','Num_of_Loan','Num_Credit_Inquiries','Credit_History_Age_Months',\n",
    "             'Changed_Credit_Limit','Credit_Mix']\n",
    "df_copy.drop(columns=[col for col in drop_cols if col in df_copy.columns], inplace=True)\n",
    "\n",
    "# Drop Customer_ID if present\n",
    "if 'Customer_ID' in df_copy.columns:\n",
    "    df_copy.drop(columns=['Customer_ID'], inplace=True)\n",
    "\n",
    "df_copy.to_csv('data/cleaned_dataset.csv', index=False)\n",
    "print(\"Final feature list:\", df_copy.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "43f954f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11216\\1558050543.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mX_train_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mX_test_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# Correlation-based feature pruning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mcorr_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[0mredundant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorr_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\MyFiles\\Apps\\Anaconda\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[0;32m  11075\u001b[0m         \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11076\u001b[0m         \u001b[0mmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11078\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"pearson\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 11079\u001b[1;33m             \u001b[0mcorrel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnancorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11080\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"spearman\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11081\u001b[0m             \u001b[0mcorrel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnancorr_spearman\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11082\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"kendall\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 1. Split Data\n",
    "# =====================================================\n",
    "X = df_copy.drop(columns=['Credit_Score'])\n",
    "y = df_copy['Credit_Score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# 2. Build Preprocessing Pipeline\n",
    "# =====================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(drop='first'), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# 3. Transform Data, Reduce Correlation\n",
    "# =====================================================\n",
    "preprocessor.fit(X_train)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "# Transform train and test sets\n",
    "X_train_t = preprocessor.transform(X_train)\n",
    "X_test_t = preprocessor.transform(X_test)\n",
    "X_train_df = pd.DataFrame(X_train_t, columns=feature_names)\n",
    "X_test_df = pd.DataFrame(X_test_t, columns=feature_names)\n",
    "\n",
    "# Correlation-based feature pruning\n",
    "corr_matrix = X_train_df.corr().abs()\n",
    "redundant = set()\n",
    "threshold = 0.9\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if corr_matrix.iloc[i, j] > threshold:\n",
    "            redundant.add(corr_matrix.columns[j])\n",
    "\n",
    "X_train_reduced = X_train_df.drop(columns=list(redundant))\n",
    "X_test_reduced = X_test_df.drop(columns=list(redundant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e09b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff996fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8223\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.80      0.77      0.79      3566\n",
      "        Poor       0.81      0.84      0.82      5799\n",
      "    Standard       0.84      0.83      0.83     10635\n",
      "\n",
      "    accuracy                           0.82     20000\n",
      "   macro avg       0.82      0.81      0.81     20000\n",
      "weighted avg       0.82      0.82      0.82     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_reduced, y_train)\n",
    "y_pred = rf.predict(X_test_reduced)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ad33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best parameters: {'max_depth': None, 'max_features': 7, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best CV score: 0.8013999779064113\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters from Randomized Search: {'n_estimators': 200, 'min_samples_split': 2, 'max_features': 7, 'max_depth': None}\n",
      "Best CV score from Randomized Search: 0.8013999779064113\n"
     ]
    }
   ],
   "source": [
    "# Grid Search with reduced features\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],  # fewer options\n",
    "    'max_depth': [8, 15, None],  # smaller set\n",
    "    'min_samples_split': [2, 10], \n",
    "    'max_features': ['auto', 7]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, verbose=2, n_jobs=4)\n",
    "grid.fit(X_train_reduced, y_train)\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best CV score:\", grid.best_score_)\n",
    "rf_best = grid.best_estimator_\n",
    "\n",
    "random = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_distributions=param_grid, n_iter=10, cv=3, verbose=2, n_jobs=4)\n",
    "random.fit(X_train_reduced, y_train)\n",
    "print(\"Best parameters from Randomized Search:\", random.best_params_)\n",
    "print(\"Best CV score from Randomized Search:\", random.best_score_)\n",
    "rf_random_best = random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6275db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8244\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.80      0.77      0.79      3566\n",
      "        Poor       0.81      0.85      0.83      5799\n",
      "    Standard       0.84      0.83      0.83     10635\n",
      "\n",
      "    accuracy                           0.82     20000\n",
      "   macro avg       0.82      0.82      0.82     20000\n",
      "weighted avg       0.82      0.82      0.82     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    'max_depth': None,\n",
    "    'max_features': 7,\n",
    "    'min_samples_split': 2,\n",
    "    'n_estimators': 200,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Initialize the model with best parameters\n",
    "rf_best = RandomForestClassifier(**best_params)\n",
    "\n",
    "# Fit the model on the full training data again\n",
    "rf_best.fit(X_train_reduced, y_train)\n",
    "y_pred = rf_best.predict(X_test_reduced)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976a79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features:\n",
      "num__Interest_Rate: 0.1133\n",
      "num__Inquiries_per_Year: 0.0991\n",
      "num__EMI_to_Income: 0.0766\n",
      "num__Total_Financial_Products: 0.0693\n",
      "num__Avg_Delay_if_Delayed: 0.0660\n",
      "num__Invest_to_Income: 0.0594\n",
      "num__Annual_Income: 0.0557\n",
      "num__Monthly_Inhand_Salary: 0.0555\n",
      "num__DTI: 0.0545\n",
      "num__Age: 0.0483\n",
      "Random Forest Cross-validation scores: [0.80975   0.8105625 0.810875  0.809125  0.8069375]\n",
      "Mean CV accuracy: 0.80945\n"
     ]
    }
   ],
   "source": [
    "# Feature importances\n",
    "importances = rf.feature_importances_\n",
    "feat_ranks = sorted(zip(X_train_reduced.columns, importances), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 Features:\")\n",
    "for f, v in feat_ranks[:10]:\n",
    "    print(f\"{f}: {v:.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf, X_train_reduced, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Random Forest Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit scoring project: End-to-End code complete.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 6. Save Model and Feature List For Deployment\n",
    "# =====================================================\n",
    "import joblib\n",
    "\n",
    "joblib.dump(rf_best, \"E:/New folder/Davv/ML/mlproject/models/credit_rf_model.pkl\")\n",
    "joblib.dump(X_train_reduced.columns.tolist(), \"feature_columns.pkl\")\n",
    "\n",
    "def predict_new(data_row):\n",
    "    model = joblib.load(\"E:/New folder/Davv/ML/mlproject/models/credit_rf_model.pkl\")\n",
    "    feature_list = joblib.load(\"feature_columns.pkl\")\n",
    "    X_new = pd.DataFrame([data_row])[feature_list]\n",
    "    pred = model.predict(X_new)[0]\n",
    "    class_map = {0: 'Poor', 1: 'Standard', 2: 'Good'}\n",
    "    return class_map.get(int(pred), pred)\n",
    "\n",
    "print(\"Credit scoring project: End-to-End code complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f2c240",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'Credit-Builder Loan', 'Mortgage Loan', 'Auto Loan', 'Limit_Decrease_Flag', 'High_Utilization', 'DTI', 'Personal Loan', 'Has_Delays', 'EMI_to_Income', 'Avg_Delay_if_Delayed', 'Student Loan', 'Inquiries_per_Year', 'Not Specified', 'Balance_to_Income', 'Total_Financial_Products', 'Home Equity Loan', 'Debt Consolidation Loan', 'Large_Limit_Change', 'Payday Loan', 'Num_Loan_Types', 'Invest_to_Income'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m raw_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([sample_raw_input])\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Transform raw numeric and categorical features using the original preprocessor\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m X_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Get the transformed feature names (numeric scaled + one-hot encoded categorical)\u001b[39;00m\n\u001b[0;32m     90\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[1;32me:\\MyFiles\\Apps\\Anaconda\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32me:\\MyFiles\\Apps\\Anaconda\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1085\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     diff \u001b[38;5;241m=\u001b[39m all_names \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m-> 1085\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1087\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: columns are missing: {'Credit-Builder Loan', 'Mortgage Loan', 'Auto Loan', 'Limit_Decrease_Flag', 'High_Utilization', 'DTI', 'Personal Loan', 'Has_Delays', 'EMI_to_Income', 'Avg_Delay_if_Delayed', 'Student Loan', 'Inquiries_per_Year', 'Not Specified', 'Balance_to_Income', 'Total_Financial_Products', 'Home Equity Loan', 'Debt Consolidation Loan', 'Large_Limit_Change', 'Payday Loan', 'Num_Loan_Types', 'Invest_to_Income'}"
     ]
    }
   ],
   "source": [
    "# raw_input contains original raw feature names; e.g. 'Age', 'Occupation'\n",
    "sample_raw_input = {\n",
    "    'ID': 'ID_0001',\n",
    "    'Customer_ID': 'CUS_0001',\n",
    "    'Month': 'November',                  # Will be converted to month number during preprocessing\n",
    "    'Name': 'John Doe',\n",
    "    'Age': 35,\n",
    "    'SSN': '123-45-6789',\n",
    "    'Occupation': 'Manager',              # Categorical\n",
    "    'Annual_Income': 60000,\n",
    "    'Monthly_Inhand_Salary': 5000.0,\n",
    "    'Num_Bank_Accounts': 3,\n",
    "    'Num_Credit_Card': 2,\n",
    "    'Interest_Rate': 10,\n",
    "    'Num_of_Loan': 1,\n",
    "    'Type_of_Loan': 'Personal Loan',     # Multi-label possible but example single string\n",
    "    'Delay_from_due_date': 5,\n",
    "    'Num_of_Delayed_Payment': 1,\n",
    "    'Changed_Credit_Limit': 1000,\n",
    "    'Num_Credit_Inquiries': 2,\n",
    "    'Credit_Mix': 'Standard',\n",
    "    'Outstanding_Debt': 15000.0,\n",
    "    'Credit_Utilization_Ratio': 0.6,\n",
    "    'Credit_History_Age': '5 Years and 3 Months',  # Will be converted to months\n",
    "    'Payment_of_Min_Amount': 'Yes',\n",
    "    'Total_EMI_per_month': 800.0,\n",
    "    'Amount_invested_monthly': 2000.0,\n",
    "    'Payment_Behaviour': 'Low_spent_Medium_value_payments',\n",
    "    'Monthly_Balance': 3500.0,\n",
    "    'Credit_Score': 'Good'                # Target variable\n",
    "}\n",
    "\n",
    "\n",
    "# Convert to DataFrame with original raw feature names\n",
    "raw_df = pd.DataFrame([sample_raw_input])\n",
    "\n",
    "# Transform raw numeric and categorical features using the original preprocessor\n",
    "X_transformed = preprocessor.transform(raw_df)\n",
    "\n",
    "# Get the transformed feature names (numeric scaled + one-hot encoded categorical)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame with these feature names\n",
    "X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "\n",
    "# Drop columns marked as redundant (make sure this set was saved from training)\n",
    "X_transformed_df_reduced = X_transformed_df.drop(columns=redundant, errors='ignore')\n",
    "\n",
    "# Now convert to dict and pass to prediction\n",
    "input_dict = X_transformed_df_reduced.iloc[0].to_dict()\n",
    "result = predict_new(input_dict)\n",
    "print(\"Predicted Credit Score:\", result)\n",
    "\n",
    "sample_df = pd.DataFrame([sample_raw_input])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
